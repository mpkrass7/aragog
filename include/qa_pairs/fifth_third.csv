requierment,response
Feature registration to keep track of published features/groups and their metadata,"DataRobot does not have a formal Feature Store; DataRobot would be complementary to the value of a Feature Store by extending the governance and monitoring of models beyond just on the Features themselves. The data assets used for modeling, including the feature engineering done as part of the modeling process, are fully documented with lineage and can be reused for future modeling efforts."
Time travel to pull accurate historical data (that matches what is in production) ,"All datasets stored in DataRobot's AI Catalog are version controlled with point-in-time snapshotting, allowing users to access or return to previous versions of the data. "
Seamless process from development to production (I want to avoid re-work),"This is one of DataRobot's core value propositions. Every model built on DataRobot uses production-ready Blueprints, so there is no rework when it's time to deploy a model to production. Feature engineering that users do in DataRobot - whether manual feature creation or via Automated Feature Discovery - is automatically recreated when the model is scored. Even when users import custom pre-processing steps or predictors, DataRobot ensures these components are production-ready and can be easily deployed."
Feature monitoring to know if/when feature is good to be used (Data quality),"Data Quality Assessment is run automatically on features brought into DataRobot for modeling. After running an extensive list of quality checks (outliers, inliers, excess zeros, disguised missing values, target leakage, irregular time series, etc.), DataRobot will flag violations for users to let them inspect features that might have data quality issues needing attention. In addition to informing the user, DataRobot will handle many of these issues automatically by adding appropriate flags or pre-processing steps to the modeling pipeline. There are additional Data Quality checks run for Time Series and Visual AI projects. 

A Data Quality Handling Report can be automatically generated for any model and provided to MRM teams. "
Data Governance to control who uses the features and how they are used,"DataRobot's AI Catalog provides governance on modeling datasets, including managing metadata, managing feature lists, seeing version history, adding comments/tags, and sharing assets with other users via Role-Based Access Control (RBAC)."
"Support code first interaction
","DataRobot has a full REST API as well as R and Python clients to allow users to access DataRobot from their notebook of choice, whether that's Jupyter or DataRobot's Notebook option."
ADDITIONAL FUNCTIONALITY: Feature Discovery,"To compliment the Feature Store, DataRobot can automate the discovery of new predictive features across linked datasets, which can significantly accelerate the process of EDA and model-building. These auto-generated features are fully documented with lineage, meaning they can then be evaluated for relevance by Data Scientists and SMEs. Features created with Feature Discovery are production-ready, which means no additional lift to recreate these features for prediction. The SQL code used to generate the features is available for download. For Snowflake users, Feature Discovery can take place entirely within Snowflake, allowing users to take advantage of the additional scale and security of Snowflake."
ADDITIONAL FUNCTIONALITY: Registry for Custom Transform Tasks,Users can create their own custom Transform tasks using Python or other tools. These custom Transform tasks can be documented and stored in the Model Registry and can then be shared to other users and reused in future DataRobot modeling projects. This ensures that custom preprocessing steps can be documented and consistently applied across modeling exercises.
Integration with GitHub for version control and collaboration,"For model deployment, DataRobot offers GitHub integration to allow users to manage custom models and their associated deployments in DataRobot via GitHub CI/CD workflows. These workflows allow you to create or delete models and deployments and modify settings via a configured GitHub workflow. Every change to your custom models can be tracked and documented. "
Ability to share projects with users and other LOBs,Projects can be easily shared via RBAC with other DataRobot users or configured groups. 
"Ability to execute Python, R, SQL Code for data manipulation and model building in IDE environment","DataRobot's managed Notebooks come with built-in R and Python environments, allowing code-first users to build, evaluate, and deploy models without needing a separate notebook solution (although users can always use other notebooks if they prefer). DataRobot Notebooks come with pre-built code snippets to help users save time, as well as OpenAI Code Assistant to help accelerate the model development process.

SQL users can also use SparkSQL directly for data transformation and feature engineering in the AI Catalog. "
Create dashboards and visualization tools (R Studio preferred) that can be published to non-platform users,"Users can easily create, manage, and share No-Code AI Apps directly from a model or a deployment. These starter templates can be easily configured to show different charts, graphs, or ""what-if"" style prediction testing. AI Apps are accessible by non-platform users.

For code-first users who want even more flexibility, DataRobot recently announced a partnership with streamlit, allowing users to..."
"Integration with other applications/platforms across enterprise (SF, DB connections, SAS, etc.) ","DataRobot allows users to create Database Connections to an extensive list of database systems via JDBC. 

For Snowflake specifically, the integration goes much deeper than just a Database Connection - DataRobot models can be deployed directly into Snowflake, allowing users to take advantage of Snowflake's scale and data protection benefits.

We do nothing with SAS now, right? "
Ability to debug and test code,"Models built using DataRobot are guaranteed to be production-ready, so most users find that their testing framework looks different for these models. For custom models built in Python, R, or Java and then deployed to DataRobot prediction servers, we offer an extensive automated testing framework to ensure that those models are production-ready. This testing includes confirming that the model makes predictions successfully, but also looks for things like whether the model can impute null values when required, whether there are side effects of different scoring configurations (e.g., models where a batch prediction returns a different result than a single row prediction), and ensuring that the model is sufficiently stable and performant. This testing can be done in the DataRobot Custom Model Workshop or locally via DataRobot Model Runner (DRUM) tool."
Streamlined process from model development to model deployment,"This is a core value proposition of DataRobot. DataRobot provides production-ready blueprints that encode best practice data science across a wide variety of modeling approaches. This means that when a data scientist signs off on a model after their process of iteration, tuning, etc., the model can be deployed to production with just a few clicks (or via the API). Every Deployment offers both batch and real-time scoring, with extensive model management, governance, and automatic alerts for data drift, accuracy, service health, and bias and fairness goals.

If DataRobot's prediction servers aren't the right option for a certain use case, DataRobot models can also be exported to run in other environments (e.g., within Snowflake, as a Java executable, Python scoring code, or as a fully containerized Portable Prediction Server). Model monitoring can also be configured for models running fully outside of DataRobot - one pane of glass across all of your models in production, no matter what tools were used to create them or what environment they're running in. "
Automatic scheduling and orchestration to manage model pipelines ,"Batch jobs can be automatically scheduled and managed via Job Definitions, which allow users to specify an input and output location and then run scoring pipelines on a predefined schedule. "
Re-create model experiments and track iteration metrics/code ,"Model experiments within a project are tracked via the Model Leaderboard. The Leaderboard provides a wealth of summary information for each model built in a project. When models complete, DataRobot lists them on the Leaderboard with performance metrics, a description and documentation, feature insights, prediction explanations, and other information for data scientists to evaluate. Every experiment (testing out a new feature list, hyperparameter tuning, changing the pre-processing steps, etc.) generates a new model on the leaderboard, so it's always easy to compare performance. Leaderboard data can of course be accessed via the API for code-driven modeling workflows. 

When working on a broader Use Case, it is common to need multiple projects as the user iterates on different problem framings and approaches. Using Workbench, users can create and manage experiments and keep all assets (notebooks, datasets, experiments) together under the umbrella of a single Use Case. "
Model Deployment integration with other applications/platforms ,"Deployed models can be scored via the UI as well as Batch and Real-Time API endpoints. Scheduled batch jobs can be easily configured, as described above. Models can also be exported as a Java executable, directly into Snowflake, as a containerized Portable Prediction Server, or as Python scoring code. (Note: not all models support all options.) Examples of how to make Batch predictions with Azure Blob storage and Google Cloud are provided. "
Model Deployment API (app creation) ,"Cory to confirm with Madison what this means - is this just ""can you deploy models via API?"""
Model registry to track models in production ,"The Model Registry is an organizational hub for the variety of models used in DataRobot. Models are registered as deployment-ready model packages; the registry lists each package available for use. Each package functions the same way, regardless of the origin of its model. In the Model Registry, you can generate model compliance documentation from model packages and deploy, share, or archive models. The Model Registry also contains the Custom Model Workshop, where you can create, deploy, and register custom models.

To track models in production, DataRobor uses the Deployment Inventory. Once models are deployed, the deployment inventory is the central hub for deployment management activity. It serves as a coordination point for all stakeholders involved in operationalizing models. From the inventory, you can monitor deployed model performance and take action as necessary, as it provides an interface to all actively deployed models, whether they are deployed on DataRobot prediction servers or in other environments. Users can easily set up alerts on these deployments, run predictions in batch or real-time formats, test out the performance of challenger models, or configure automatic retraining."
Ability to freeze environments and replicate for model building/scoring,"For DataRobot models, this is managed by DataRobot. For custom models brought into the Model Registry, users can specify one of the provided drop-in environments for deployment and then upload a requirements file to generate a custom scoring environment. This allows organizations to easily deploy many models with competing environments. Environments can also be easily reused for future models."
Model pipelines can be shared with other users and LOBs,DataRobot modeling projects as well as models added to the Model Registry and any user-defined custom tasks loaded into DataRobot can be easily shared to other DataRobot users.
Workflows to keep track of MRM processes and status (assign tasks to users),"There is a built-in approval workflow for new deployments as well as making edits to existing deployments. When you create a new or change an existing deployment, if the approval workflow is enabled, an MLOps or MRM administrator within your organization must approve your changes. Once the deployment is created, Admins are alerted via email that the deployment requires review. While awaiting review, the deployment is flagged as ""NEEDS APPROVAL"" in the deployment inventory.

Approval policies affect the users who have permissions to review deployments, and provide automated actions when reviews time out. Approval policies also affect users whose deployment events are governed by a configured policy (e.g., new deployment creation, model replacement)."
Generate Automated Compliance Documentation?,"DataRobot generates automated compliance documentation for models built on DataRobot as well as for custom models ingested to the DataRobot Model Registry. Automated Compliance Docs are a detailed 30+ page overview of the modeling process, the relative importance of each feature included in the model, the model performance details etc. Users can also load and configure their own organization's templates to further expedite MRM review. "
"Resources: Having what we need when we need it (scalability, static reserve etc.) ","Your license in DataRobot will govern how many concurrent modeling workers the organization has access to. DataRobot manages the queue, if there is one. Users submit modeling jobs and they will run when those users have workers available to run them. Prediction servers do not use modeling workers, so model development does not affect prediction capacity. Prediction servers can be configured to be High-Availability depending on customer needs. 

DataRobot is Kubernetes-based. For self-managed installations (i.e., deployment to a VPC), workers autoscale and ramp down when they are unused to save cloud compute cost. "
"Resource monitoring
","DataRobot offers a Resource Monitor to help organizations manage usage across teams. This allows administrators to see how many modeling workers are being in use, how many jobs are queued, etc. 

Note that DataRobot's pricing model is a fixed license cost - we do not charge for compute, so there is no need for a Cost Monitor."
Singular platform that hosts capabilities in one-stop shop,"DataRobot is one of the only providers in the ML Production space that offers the ability to monitor models deployed outside of our environment - this makes it a great solution for hybrid organizations that may have multiple different deployment environments or competing cloud investments

The Deployment Inventory is an ML Production hub that reduces an organization's exposure to risk, increases the efficiency of the data science team, and empowers MRM to ensure that models are working as expected. DataRobot provides a one-stop shop for model building and production while also allowing for the flexibility that highly complex organizations and teams need.

To give you a few examples, here are four scenarios of different needs around model building and deployment, all of which could be tracked via the Deployment Inventory.
- Scenario 1: A user wants to quickly iterate on a new use case and then rapidly productionalize the model with real-time or batch scoring 
- Scenario 2: An expert data scientist user (who does not have full-stack skills) has developed a custom model in Python or R, for which they need a consistent and scalable deployment option. This model would also go through a thorough testing and validation process to ensure that it works as expected in production. 
- Scenario 3: For a use case with highly specific needs around security, a team wants to use DataRobot automation to build and document a model, but then wants to deploy the model directly into Snowflake to ensure that production data never leaves that environment
- Scenario 4: A user has a legacy model in production that has been developed and deployed entirely outside of DataRobot, but would like to reduce the time spent actively monitoring its performance. DataRobot Monitoring Agents could monitor the model in-place, no need to retrain or redeploy."